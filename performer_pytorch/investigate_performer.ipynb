{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Performer \n",
    "-  https://github.com/lucidrains/performer-pytorch/tree/main/performer_pytorch\n",
    "- https://arxiv.org/abs/2009.14794\n",
    "\n",
    "Performer is a Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. To approximate softmax attention-kernels, Performers use a Fast Attention Via positive Orthogonal Random features approach (FAVOR+), leveraging new methods for approximating softmax and Gaussian kernels.\n",
    "\n",
    "- BERT - short for Bidirectional Encoder Representations from Transformers\n",
    "\n",
    " - DistilBERT offers a lighter version of BERT; runs 60% faster while maintaining over 95% of BERT’s performance.\n",
    "\n",
    "- Masked Language Model - enables/enforces bidirectional learning from text by masking (hiding) a word in a sentence and forcing BERT to bidirectionally use the words on either side of the covered word to predict the masked word.\n",
    "\n",
    "- BERT can be more accurate than humans when predicting the missing word. \n",
    "\n",
    "- A random 15% of tokenized words are hidden during training and BERT’s job is to correctly predict the hidden words\n",
    "\n",
    "- BERT is trained on both MLM masked language model (50%) and NSP next sentence prediction (50%) at the same time. \n",
    "\n",
    "- Transformers use an attention mechanism to observe relationships between words.\n",
    "\n",
    "##### Pretraining:\n",
    "BERT is initially pretrained on a large corpus of text using two unsupervised tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). This phase enables BERT to understand language patterns, word relationships, and contextual information broadly. Essentially, it builds a foundational language understanding that can be applied across various tasks.\n",
    "\n",
    "##### Fine-Tuning:\n",
    "After pretraining, BERT is fine-tuned on a specific task (like sentiment analysis, question answering, etc.). Fine-tuning adjusts the pretrained model to perform well on the target task by training on labeled data specific to that task. This helps the model apply its broad language understanding to the particular nuances and requirements of the target application.\n",
    "\n",
    "In essence, pretraining provides BERT with a deep, generalized understanding of language, while fine-tuning tailors this understanding to excel in specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
